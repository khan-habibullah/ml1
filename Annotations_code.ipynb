{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khan-habibullah/ml1/blob/master/Annotations_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAy6Ljdktgnv",
        "outputId": "1f11647e-8fb0-4e4b-a535-6911a2b55b2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'frozen_east_text_detection.pb' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/oyyd/frozen_east_text_detection.pb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "vYR6Fy09m-kR"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import glob\n",
        "\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "def decode(scores, geometry):\n",
        "    detections = []\n",
        "    confidences = []\n",
        "    height, width = scores.shape[2:4]\n",
        "    for y in range(0, height):\n",
        "        scores_data = scores[0][0][y]\n",
        "        x0_data = geometry[0][0][y]\n",
        "        x1_data = geometry[0][1][y]\n",
        "        x2_data = geometry[0][2][y]\n",
        "        x3_data = geometry[0][3][y]\n",
        "        angles_data = geometry[0][4][y]\n",
        "        for x in range(0, width):\n",
        "            score = scores_data[x]\n",
        "            if score < 0.5:\n",
        "                continue\n",
        "            offset_x = x * 4.0\n",
        "            offset_y = y * 4.0\n",
        "            angle = angles_data[x]\n",
        "            cos_a = np.cos(angle)\n",
        "            sin_a = np.sin(angle)\n",
        "            h = x0_data[x] + x2_data[x]\n",
        "            w = x1_data[x] + x3_data[x]\n",
        "            end_x = int(offset_x + (cos_a * x1_data[x]) + (sin_a * x2_data[x]))\n",
        "            end_y = int(offset_y - (sin_a * x1_data[x]) + (cos_a * x2_data[x]))\n",
        "            start_x = int(end_x - w)\n",
        "            start_y = int(end_y - h)\n",
        "            detections.append((start_x, start_y, end_x, end_y))\n",
        "            confidences.append(score)\n",
        "    boxes = non_max_suppression(np.array(detections), probs=confidences, overlapThresh=0.5)\n",
        "    return boxes\n",
        "\n",
        "def non_max_suppression(boxes, probs=None, overlapThresh=0.3):\n",
        "    if len(boxes) == 0:\n",
        "        return []\n",
        "    if boxes.dtype.kind == \"i\":\n",
        "        boxes = boxes.astype(\"float\")\n",
        "    pick = []\n",
        "    x1 = boxes[:, 0]\n",
        "    y1 = boxes[:, 1]\n",
        "    x2 = boxes[:, 2]\n",
        "    y2 = boxes[:, 3]\n",
        "    area = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
        "    if probs is not None:\n",
        "        idxs = np.argsort(probs)\n",
        "    else:\n",
        "        idxs = np.argsort(y2)\n",
        "    while len(idxs) > 0:\n",
        "        last = len(idxs) - 1\n",
        "        i = idxs[last]\n",
        "        pick.append(i)\n",
        "        suppress = [last]\n",
        "        for pos in range(0, last):\n",
        "            j = idxs[pos]\n",
        "            xx1 = max(x1[i], x1[j])\n",
        "            yy1 = max(y1[i], y1[j])\n",
        "            xx2 = min(x2[i], x2[j])\n",
        "            yy2 = min(y2[i], y2[j])\n",
        "            w = max(0, xx2 - xx1 + 1)\n",
        "            h = max(0, yy2 - yy1 + 1)\n",
        "            overlap = float(w * h) / area[j]\n",
        "            if overlap > overlapThresh:\n",
        "                suppress.append(pos)\n",
        "        idxs = np.delete(idxs, suppress)\n",
        "    return boxes[pick].astype(\"int\")\n",
        "\n",
        "\n",
        "\n",
        "def resize_image(image, height=640, width=640):\n",
        "    \"\"\"Resizes an input image to the required size of the EAST text detection model.\"\"\"\n",
        "    # Get the original image dimensions\n",
        "    orig_height, orig_width = image.shape[:2]\n",
        "\n",
        "    # Calculate the ratio of the original dimensions to the desired dimensions\n",
        "    height_ratio = height / float(orig_height)\n",
        "    width_ratio = width / float(orig_width)\n",
        "\n",
        "    # Determine which ratio to use for resizing\n",
        "    ratio = min(height_ratio, width_ratio)\n",
        "\n",
        "    # Calculate the new dimensions\n",
        "    new_height = int(orig_height * ratio)\n",
        "    new_width = int(orig_width * ratio)\n",
        "\n",
        "    # Resize the image\n",
        "    resized_image = cv2.resize(image, (new_width, new_height))\n",
        "\n",
        "    # Pad the image to the required size\n",
        "    delta_w = width - new_width\n",
        "    delta_h = height - new_height\n",
        "    top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n",
        "    left, right = delta_w // 2, delta_w - (delta_w // 2)\n",
        "    padded_image = cv2.copyMakeBorder(resized_image, top, bottom, left, right, cv2.BORDER_CONSTANT, value=(0, 0, 0))\n",
        "\n",
        "    #Return the resized and padded image\n",
        "    return padded_image, top, bottom, left, right\n",
        "\n",
        "\n",
        "def get_text(image_path, net):\n",
        "\n",
        "  # Define the path to the pre-trained EAST model\n",
        "\n",
        "  # Load the input image\n",
        "  image = cv2.imread(image_path)\n",
        "  height = image.shape[0]\n",
        "  width  = image.shape[1]\n",
        "  new_height = 640\n",
        "  new_width = 640\n",
        "  image, top, bottom, left, right = resize_image(image, height=new_height, width=new_width)\n",
        "  w_ratio = width / (new_width - left - right)\n",
        "  h_ratio = height / (new_height - top - bottom)\n",
        "  # Get the height and width of the image\n",
        "  height, width, _ = image.shape\n",
        "\n",
        "  # Load the pre-trained EAST model\n",
        "  # Create a blob from the image and set the input for the EAST model\n",
        "  blob = cv2.dnn.blobFromImage(image, scalefactor=1.0, size=(width, height), mean=(123.68, 116.78, 103.94), swapRB=True, crop=False)\n",
        "  net.setInput(blob)\n",
        "  # Get the output from the EAST model\n",
        "  scores, geometry = net.forward([\"feature_fusion/Conv_7/Sigmoid\", \"feature_fusion/concat_3\"])\n",
        "  # Decode the output to get the bounding boxes of the detected text\n",
        "  boxes = decode(scores, geometry)\n",
        "  # Draw the bounding boxes on the image\n",
        "\n",
        "  bbox_list  = []\n",
        "  for box in boxes:\n",
        "      x1, y1, x2, y2 = map(int, box)\n",
        "      x1 = (x1 - left) * w_ratio\n",
        "      x2 = (x2 - left) * w_ratio\n",
        "      \n",
        "      y1 = (y1 - top) * h_ratio\n",
        "      y2 = (y2 - top) * h_ratio\n",
        "      bbox_list.append({\"class\":\"text\",\"bbox\":[x1, y1, x2, y2]})\n",
        "      # o_im = cv2.imread(image_path)\n",
        "      # cv2.rectangle(o_im, (int(x1), int(y1)), (int(x2), int(y2)), (0, 255, 0), 2)\n",
        "      # cv2.imwrite(\"arsla.png\",o_im)\n",
        "  return bbox_list\n",
        "\n",
        "\n",
        "\n",
        "def get_dict_for_annotations():\n",
        "    return {\n",
        "        \"_via_settings\": {\n",
        "            \"ui\": {\n",
        "                \"annotation_editor_height\": 25,\n",
        "                \"annotation_editor_fontsize\": 0.8,\n",
        "                \"leftsidebar_width\": 18,\n",
        "                \"image_grid\": {\n",
        "                    \"img_height\": 80,\n",
        "                    \"rshape_fill\": \"none\",\n",
        "                    \"rshape_fill_opacity\": 0.3,\n",
        "                    \"rshape_stroke\": \"yellow\",\n",
        "                    \"rshape_stroke_width\": 2,\n",
        "                    \"show_region_shape\": True,\n",
        "                    \"show_image_policy\": \"all\"\n",
        "                },\n",
        "                \"image\": {\n",
        "                    \"region_label\": \"class\",\n",
        "                    \"region_color\": \"__via_default_region_color__\",\n",
        "                    \"region_label_font\": \"10px Sans\",\n",
        "                    \"on_image_annotation_editor_placement\": \"NEAR_REGION\"\n",
        "                }\n",
        "            },\n",
        "            \"core\": {\n",
        "                \"buffer_size\": 18,\n",
        "                \"filepath\": {},\n",
        "                \"default_filepath\": \"\"\n",
        "            },\n",
        "            \"project\": {\n",
        "                \"name\": \"idcard_annotations\"\n",
        "            }\n",
        "        },\n",
        "        \"_via_img_metadata\": {},\n",
        "        \"_via_attributes\": {\n",
        "            \"region\": {\n",
        "                \"class\": {\n",
        "                    \"type\": \"checkbox\",\n",
        "                    \"description\": \"\",\n",
        "                    \"options\": {'\"\"': ''},\n",
        "                    \"default_options\": {}\n",
        "                }\n",
        "            },\n",
        "            \"file\": {}\n",
        "        }\n",
        "    }\n",
        "\n",
        "\n",
        "def bboxes_to_via_annotations(bboxes):\n",
        "    via_annotaitons_data = get_dict_for_annotations()\n",
        "    for im_path, bbox in bboxes.items():\n",
        "        im_name = im_path.split(\"/\")[-1]\n",
        "        im_size = os.path.getsize(im_path)\n",
        "        im_key = im_name + str(im_size)\n",
        "        regions = []\n",
        "        for b in bbox:\n",
        "            regions += [\n",
        "                {\n",
        "                    \"shape_attributes\": {\n",
        "                        \"name\": \"rect\",\n",
        "                        \"x\": b[\"bbox\"][0],\n",
        "                        \"y\": b[\"bbox\"][1],\n",
        "                        \"width\": b[\"bbox\"][2] - b[\"bbox\"][0],\n",
        "                        \"height\": b[\"bbox\"][3] - b[\"bbox\"][1]\n",
        "                    },\n",
        "                    \"region_attributes\": {\n",
        "                        \"class\": {\n",
        "                            b[\"class\"]: True\n",
        "                        }\n",
        "                    }\n",
        "                }\n",
        "            ]\n",
        "            via_annotaitons_data[\"_via_attributes\"][\"region\"][\"class\"][\"options\"][b[\"class\"]] = \"\"\n",
        "        via_annotaitons_data[\"_via_img_metadata\"][im_key] = {\n",
        "            \"filename\": im_name,\n",
        "            \"size\": im_size,\n",
        "            \"regions\": regions,\n",
        "            \"file_attributes\": {}\n",
        "        }\n",
        "    return via_annotaitons_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KJL81Thnjmv",
        "outputId": "ea483262-b655-4345-960f-5d9ec98d8c0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 145/145 [03:22<00:00,  1.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "im_dir_path = \"/content/Arabicid\"\n",
        "\n",
        "\n",
        "model_path = '/content/frozen_east_text_detection.pb/frozen_east_text_detection.pb'\n",
        "net = cv2.dnn.readNet(model_path)\n",
        "\n",
        "\n",
        "im_list = glob.glob(os.path.join(im_dir_path,\"*.*g\")) + glob.glob(os.path.join(im_dir_path,\"*.*G\"))\n",
        "\n",
        "import tqdm\n",
        "bboxes = {}\n",
        "for im_path in tqdm.tqdm(im_list):\n",
        "    bboxes[im_path] = get_text(im_path, net)\n",
        "\n",
        "\n",
        "via_annotaitons_data = bboxes_to_via_annotations(bboxes)\n",
        "json.dump(via_annotaitons_data, open(\"annotations.json\",\"w\"))\n",
        "print(\"done\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ceCjJ9TZtBFe"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPgJLVzA97ip1+Y1PnBPXUB",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}